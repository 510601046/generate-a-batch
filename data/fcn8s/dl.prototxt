name: "nullmax-detection-seg-net"
layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape: {
      dim: 1
      dim: 3
      dim: 416
      dim: 640
    }
  }
}
layer {
  name: "scale_data_lane"
  type: "Power"
  bottom: "data"
  top: "scale_data_lane"
  power_param {
      power: 1
      scale: 0.00392157
      shift: 0
  }
  propagate_down: false

}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "scale_data_lane"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv1_bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"

  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv1_scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv1_relu"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    pad: 0
    stride: 2
  }
}

layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv2_bn"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv2_scale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv2_relu"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    pad: 0
    stride: 2
  }
}

layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv3_1_bn"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
 
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv3_1_scale"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv3_1_relu"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv3_2_bn"
  type: "BatchNorm"
  bottom: "conv3_2"
  top: "conv3_2"
 
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv3_2_scale"
  type: "Scale"
  bottom: "conv3_2"
  top: "conv3_2"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv3_2_relu"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv3_3_bn"
  type: "BatchNorm"
  bottom: "conv3_3"
  top: "conv3_3"
 
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv3_3_scale"
  type: "Scale"
  bottom: "conv3_3"
  top: "conv3_3"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv3_3_relu"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    pad: 0
    stride: 2
  }
}

layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv4_1_bn"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv4_1_scale"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv4_1_relu"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv4_2_bn"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv4_2_scale"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv4_2_relu"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv4_3_bn"
  type: "BatchNorm"
  bottom: "conv4_3"
  top: "conv4_3"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv4_3_scale"
  type: "Scale"
  bottom: "conv4_3"
  top: "conv4_3"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv4_3_relu"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_3"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    pad: 0
    stride: 2
  }
}

layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv5_1_bn"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "conv5_1"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv5_1_scale"
  type: "Scale"
  bottom: "conv5_1"
  top: "conv5_1"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv5_1_relu"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv5_2_bn"
  type: "BatchNorm"
  bottom: "conv5_2"
  top: "conv5_2"
 
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv5_2_scale"
  type: "Scale"
  bottom: "conv5_2"
  top: "conv5_2"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv5_2_relu"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv5_3_bn"
  type: "BatchNorm"
  bottom: "conv5_3"
  top: "conv5_3"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv5_3_scale"
  type: "Scale"
  bottom: "conv5_3"
  top: "conv5_3"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv5_3_relu"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv5_4"
  type: "Convolution"
  bottom: "conv5_3"
  top: "conv5_4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv5_4_bn"
  type: "BatchNorm"
  bottom: "conv5_4"
  top: "conv5_4"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv5_4_scale"
  type: "Scale"
  bottom: "conv5_4"
  top: "conv5_4"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv5_4_relu"
  type: "ReLU"
  bottom: "conv5_4"
  top: "conv5_4"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv5_5"
  type: "Convolution"
  bottom: "conv5_4"
  top: "conv5_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv5_5_bn"
  type: "BatchNorm"
  bottom: "conv5_5"
  top: "conv5_5"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv5_5_scale"
  type: "Scale"
  bottom: "conv5_5"
  top: "conv5_5"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv5_5_relu"
  type: "ReLU"
  bottom: "conv5_5"
  top: "conv5_5"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5_5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    pad: 1
    stride: 1
  }
}

layer {
  name: "conv6_1"
  type: "Convolution"
  bottom: "pool5"
  top: "conv6_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 2
    kernel_size: 5
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv6_1_bn"
  type: "BatchNorm"
  bottom: "conv6_1"
  top: "conv6_1"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv6_1_scale"
  type: "Scale"
  bottom: "conv6_1"
  top: "conv6_1"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv6_1_relu"
  type: "ReLU"
  bottom: "conv6_1"
  top: "conv6_1"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv6_2"
  type: "Convolution"
  bottom: "conv6_1"
  top: "conv6_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv6_2_bn"
  type: "BatchNorm"
  bottom: "conv6_2"
  top: "conv6_2"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv6_2_scale"
  type: "Scale"
  bottom: "conv6_2"
  top: "conv6_2"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv6_2_relu"
  type: "ReLU"
  bottom: "conv6_2"
  top: "conv6_2"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv6_3"
  type: "Convolution"
  bottom: "conv6_2"
  top: "conv6_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv6_3_bn"
  type: "BatchNorm"
  bottom: "conv6_3"
  top: "conv6_3"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv6_3_scale"
  type: "Scale"
  bottom: "conv6_3"
  top: "conv6_3"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv6_3_relu"
  type: "ReLU"
  bottom: "conv6_3"
  top: "conv6_3"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv6_4"
  type: "Convolution"
  bottom: "conv6_3"
  top: "conv6_4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv6_4_bn"
  type: "BatchNorm"
  bottom: "conv6_4"
  top: "conv6_4"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv6_4_scale"
  type: "Scale"
  bottom: "conv6_4"
  top: "conv6_4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv6_4_relu"
  type: "ReLU"
  bottom: "conv6_4"
  top: "conv6_4"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv6_5"
  type: "Convolution"
  bottom: "conv6_4"
  top: "conv6_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv6_5_bn"
  type: "BatchNorm"
  bottom: "conv6_5"
  top: "conv6_5"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv6_5_scale"
  type: "Scale"
  bottom: "conv6_5"
  top: "conv6_5"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv6_5_relu"
  type: "ReLU"
  bottom: "conv6_5"
  top: "conv6_5"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv7_1"
  type: "Convolution"
  bottom: "conv6_5"
  top: "conv7_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv7_1_bn"
  type: "BatchNorm"
  bottom: "conv7_1"
  top: "conv7_1"
 
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv7_1_scale"
  type: "Scale"
  bottom: "conv7_1"
  top: "conv7_1"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv7_1_relu"
  type: "ReLU"
  bottom: "conv7_1"
  top: "conv7_1"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv7_2"
  type: "Convolution"
  bottom: "conv7_1"
  top: "conv7_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv7_2_bn"
  type: "BatchNorm"
  bottom: "conv7_2"
  top: "conv7_2"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv7_2_scale"
  type: "Scale"
  bottom: "conv7_2"
  top: "conv7_2"
  param {                                                                                                                                                                                      
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv7_2_relu"
  type: "ReLU"
  bottom: "conv7_2"
  top: "conv7_2"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "concat8"
  type: "Concat"
  bottom: "conv5_5"
  bottom: "conv7_2"
  top: "concat8"
  concat_param {
    axis: 1
  }
}

layer {
  name: "conv9"
  type: "Convolution"
  bottom: "concat8"
  top: "conv9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv9_bn"
  type: "BatchNorm"
  bottom: "conv9"
  top: "conv9"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv9_scale"
  type: "Scale"
  bottom: "conv9"
  top: "conv9"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv9_relu"
  type: "ReLU"
  bottom: "conv9"
  top: "conv9"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "pool6"
  type: "Pooling"
  bottom: "conv9"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

layer {
  name: "conv10"
  type: "Convolution"
  bottom: "pool6"
  top: "conv10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 24
    bias_term: false
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}

#layer {
#  name: "Region_Loss_0"
#  type: "RegionLoss"
#  bottom: "conv10"
#  bottom: "annotation"
#  top: "det_loss_scale_0"
#  loss_weight: 1
#  region_loss_param {
#    num_class: 1
#    coords: 4
#    num: 5
#    max_boxes: 90
#    
#    object_scale: 5.0
#    noobject_scale: 1.0
#    class_scale: 1.0
#    coord_scale: 1.0
#    
#    thresh: 0.6
#    random: 1
#    
#    # the original size divided by 32, 196.6326,179.6987, 177.0434,302.9214, 380.4965,269.1748
#    biases: 6.1562
#    biases: 5.625
#    biases: 5.5312
#    biases: 9.46875
#    biases: 11.875
#    biases: 8.40625
#  }
#}

layer {
  name: "conv_s1_1"
  type: "Convolution"
  bottom: "conv9"
  top: "conv_s1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s1_1_bn"
  type: "BatchNorm"
  bottom: "conv_s1_1"
  top: "conv_s1_1"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s1_1_scale"
  type: "Scale"
  bottom: "conv_s1_1"
  top: "conv_s1_1"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s1_1_relu"
  type: "ReLU"
  bottom: "conv_s1_1"
  top: "conv_s1_1"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s1_2"
  type: "Convolution"
  bottom: "conv_s1_1"
  top: "conv_s1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s1_2_bn"
  type: "BatchNorm"
  bottom: "conv_s1_2"
  top: "conv_s1_2"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s1_2_scale"
  type: "Scale"
  bottom: "conv_s1_2"
  top: "conv_s1_2"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s1_2_relu"
  type: "ReLU"
  bottom: "conv_s1_2"
  top: "conv_s1_2"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s1_3"
  type: "Convolution"
  bottom: "conv_s1_2"
  top: "conv_s1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s1_3_bn"
  type: "BatchNorm"
  bottom: "conv_s1_3"
  top: "conv_s1_3"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s1_3_scale"
  type: "Scale"
  bottom: "conv_s1_3"
  top: "conv_s1_3"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s1_3_relu"
  type: "ReLU"
  bottom: "conv_s1_3"
  top: "conv_s1_3"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s1_4"
  type: "Convolution"
  bottom: "conv_s1_3"
  top: "conv_s1_4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s1_4_bn"
  type: "BatchNorm"
  bottom: "conv_s1_4"
  top: "conv_s1_4"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s1_4_scale"
  type: "Scale"
  bottom: "conv_s1_4"
  top: "conv_s1_4"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s1_4_relu"
  type: "ReLU"
  bottom: "conv_s1_4"
  top: "conv_s1_4"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s1_5"
  type: "Convolution"
  bottom: "conv_s1_4"
  top: "conv_s1_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s1_5_bn"
  type: "BatchNorm"
  bottom: "conv_s1_5"
  top: "conv_s1_5"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s1_5_scale"
  type: "Scale"
  bottom: "conv_s1_5"
  top: "conv_s1_5"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s1_5_relu"
  type: "ReLU"
  bottom: "conv_s1_5"
  top: "conv_s1_5"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s1_6"
  type: "Convolution"
  bottom: "conv_s1_5"
  top: "conv_s1_6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s1_6_bn"
  type: "BatchNorm"
  bottom: "conv_s1_6"
  top: "conv_s1_6"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s1_6_scale"
  type: "Scale"
  bottom: "conv_s1_6"
  top: "conv_s1_6"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s1_6_relu"
  type: "ReLU"
  bottom: "conv_s1_6"
  top: "conv_s1_6"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv_s1_7"
  type: "Convolution"
  bottom: "conv_s1_6"
  top: "conv_s1_7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 24
    bias_term: false
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}

#layer {
#  name: "Region_Loss_1"
#  type: "RegionLoss"
#  bottom: "conv_s1_7"
#  bottom: "annotation"
#  top: "det_loss_scale_1"
#  loss_weight: 1
#  region_loss_param {
#    num_class: 1
#    coords: 4
#    num: 5
#    max_boxes: 90
#    
#    object_scale: 5.0
#    noobject_scale: 1.0
#    class_scale: 1.0
#    coord_scale: 1.0
#    
#    thresh: 0.6
#    random: 1#

#    # divided by 16, 94.7206,80.4208, 90.3188,172.6799, 147.1237,118.1976
#    biases: 5.9375
#    biases: 5
#    biases: 5.625
#    biases: 10.8125
#    biases: 9.1875
#    biases: 7.375
#  }
#}


layer {
  name: "conv_s2_0"
  type: "Convolution"
  bottom: "conv_s1_5"
  top: "conv_s2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s2_0_bn"
  type: "BatchNorm"
  bottom: "conv_s2_0"
  top: "conv_s2_0"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s2_0_scale"
  type: "Scale"
  bottom: "conv_s2_0"
  top: "conv_s2_0"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s2_0_relu"
  type: "ReLU"
  bottom: "conv_s2_0"
  top: "conv_s2_0"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
    name: "upsample_s2_0"
    type: "Deconvolution"
    bottom: "conv_s2_0"
    top: "upsample_s2_0"
    param{
        lr_mult: 1
        decay_mult: 1
    }
    convolution_param {
      kernel_size: 2  
      stride: 2       
      num_output: 128 
      pad: 0          
      weight_filler: { type: "xavier" } 
      bias_filler {
          type: "constant"
          value: 0
      }
    }
}

#layer {
#  name: "upsample_s2_0_bn"
#  type: "BatchNorm"
#  bottom: "upsample_s2_0"
#  top: "upsample_s2_0"
#  batch_norm_param {
#    
#    eps: 1e-06
#  }
#}
#layer {
#  name: "upsample_s2_0_scale"
#  type: "Scale"
#  bottom: "upsample_s2_0"
#  top: "upsample_s2_0"
#  scale_param {
#    filler {
#      type: "constant"
#      value: 1
#    }
#    bias_term: true
#  }
#}
#layer {
#  name: "upsample_s2_0_relu"
#  type: "ReLU"
#  bottom: "upsample_s2_0"
#  top: "upsample_s2_0"
#  relu_param {
#    negative_slope: 0
#  }
#}

layer {
    bottom: "upsample_s2_0"
    bottom: "conv4_3"
    top: "concat_s2_0"
    name: "concat_s2_0"
    type: "Concat"
    concat_param {
    axis: 1
  }
}

layer {
  name: "conv_s2_1"
  type: "Convolution"
  bottom: "concat_s2_0"
  top: "conv_s2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s2_1_bn"
  type: "BatchNorm"
  bottom: "conv_s2_1"
  top: "conv_s2_1"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s2_1_scale"
  type: "Scale"
  bottom: "conv_s2_1"
  top: "conv_s2_1"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s2_1_relu"
  type: "ReLU"
  bottom: "conv_s2_1"
  top: "conv_s2_1"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s2_2"
  type: "Convolution"
  bottom: "conv_s2_1"
  top: "conv_s2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s2_2_bn"
  type: "BatchNorm"
  bottom: "conv_s2_2"
  top: "conv_s2_2"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s2_2_scale"
  type: "Scale"
  bottom: "conv_s2_2"
  top: "conv_s2_2"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s2_2_relu"
  type: "ReLU"
  bottom: "conv_s2_2"
  top: "conv_s2_2"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s2_3"
  type: "Convolution"
  bottom: "conv_s2_2"
  top: "conv_s2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s2_3_bn"
  type: "BatchNorm"
  bottom: "conv_s2_3"
  top: "conv_s2_3"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s2_3_scale"
  type: "Scale"
  bottom: "conv_s2_3"
  top: "conv_s2_3"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s2_3_relu"
  type: "ReLU"
  bottom: "conv_s2_3"
  top: "conv_s2_3"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s2_4"
  type: "Convolution"
  bottom: "conv_s2_3"
  top: "conv_s2_4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s2_4_bn"
  type: "BatchNorm"
  bottom: "conv_s2_4"
  top: "conv_s2_4"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s2_4_scale"
  type: "Scale"
  bottom: "conv_s2_4"
  top: "conv_s2_4"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s2_4_relu"
  type: "ReLU"
  bottom: "conv_s2_4"
  top: "conv_s2_4"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s2_5"
  type: "Convolution"
  bottom: "conv_s2_4"
  top: "conv_s2_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s2_5_bn"
  type: "BatchNorm"
  bottom: "conv_s2_5"
  top: "conv_s2_5"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s2_5_scale"
  type: "Scale"
  bottom: "conv_s2_5"
  top: "conv_s2_5"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s2_5_relu"
  type: "ReLU"
  bottom: "conv_s2_5"
  top: "conv_s2_5"
  relu_param {
    negative_slope: 0.0
  }
}


layer {
  name: "conv_s2_6"
  type: "Convolution"
  bottom: "conv_s2_5"
  top: "conv_s2_6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "conv_s2_6_bn"
  type: "BatchNorm"
  bottom: "conv_s2_6"
  top: "conv_s2_6"
  
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "conv_s2_6_scale"
  type: "Scale"
  bottom: "conv_s2_6"
  top: "conv_s2_6"
  param {                                                                                                                                                                              
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "conv_s2_6_relu"
  type: "ReLU"
  bottom: "conv_s2_6"
  top: "conv_s2_6"
  relu_param {
    negative_slope: 0.0
  }
}

layer {
  name: "conv_s2_7"
  type: "Convolution"
  bottom: "conv_s2_6"
  top: "conv_s2_7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 24
    bias_term: false
    kernel_size: 1
    dilation: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}

#layer {
#  name: "Region_Loss_2"
#  type: "RegionLoss"
#  bottom: "conv_s2_7"
#  bottom: "annotation"
#  top: "det_loss_scale_2"
#  loss_weight: 1
#  region_loss_param {
#    num_class: 1
#    coords: 4
#    num: 5
#    max_boxes: 90
#    
#    object_scale: 5.0
#    noobject_scale: 1.0
#    class_scale: 1.0
#    coord_scale: 1.0
#    
#    thresh: 0.6
#    random: 1#

#    # divided by 8, 13.0974,12.2877, 32.4039,28.8337, 59.1934,51.5228
#    
#    biases: 1.625
#    biases: 1.5
#    biases: 4
#    biases: 3.625
#    biases: 7.375
#    biases: 6.5
#  }
#}

######### upsample 2 ########
layer {
  name: "reduce2_lane"
  type: "Convolution"
  bottom: "conv_s2_6"
  top: "reduce2_lane"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "reduce2_lane_bn"
  type: "BatchNorm"
  bottom: "reduce2_lane"
  top: "reduce2_lane"
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "reduce2_lane_scale"
  type: "Scale"
  bottom: "reduce2_lane"
  top: "reduce2_lane"
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "reduce2_lane_relu"
  type: "ReLU"
  bottom: "reduce2_lane"
  top: "reduce2_lane"
  relu_param {
    negative_slope: 0
  }
}

layer {
    name: "deconv2_lane"
    type: "Deconvolution"
    bottom: "reduce2_lane"
    top: "deconv2_lane"
    param{
        lr_mult: 1
        decay_mult: 1
    }
    convolution_param {
      kernel_size: 2  # {{2 * factor _ factor % 2}}  2 * 2 _ 0
      stride: 2       # {{factor}}
      num_output: 32 # {{C}} 
      #group: 48      # {{C}}
      pad: 0          # {{ceil((factor _ 1) / 2.)}} 2 _ 1 / 2
      weight_filler: { type: "xavier" } 
      bias_filler {
          type: "constant"
          value: 0
      }
    }
}

layer {
  name: "deconv2_lane_bn"
  type: "BatchNorm"
  bottom: "deconv2_lane"
  top: "deconv2_lane"
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "deconv2_lane_scale"
  type: "Scale"
  bottom: "deconv2_lane"
  top: "deconv2_lane"
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "deconv2_lane_relu"
  type: "ReLU"
  bottom: "deconv2_lane"
  top: "deconv2_lane"
  relu_param {
    negative_slope: 0
  }
}


########## end upsample 2 #####

########### upsample 3

layer {
  name: "reorg3"
  type: "Convolution"
  bottom: "conv3_3"
  top: "reorg3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    kernel_size: 3
    pad: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
##layer {
## name: "reorg3_bn"
## type: "BatchNorm"
## bottom: "reorg3"
## top: "reorg3"
## batch_norm_param {
##   eps: 1e-06
## }
##}
##layer {
## name: "reorg3_scale"
## type: "Scale"
## bottom: "reorg3"
## top: "reorg3"
## scale_param {
##   filler {
##     type: "constant"
##     value: 1
##   }
##   bias_term: true
## }
##}
layer {
  name: "reorg3_relu"
  type: "ReLU"
  bottom: "reorg3"
  top: "reorg3"
  relu_param {
    negative_slope: 0
  }
}


layer {
    type: "Concat"
    name: "concat3"
    bottom: "reorg3"
    bottom: "deconv2_lane"
    top: "concat3"

    concat_param{
      axis: 1
    }  
}

layer {
  name: "reduce3_lane"
  type: "Convolution"
  bottom: "concat3"
  top: "reduce3_lane"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "reduce3_lane_bn"
  type: "BatchNorm"
  bottom: "reduce3_lane"
  top: "reduce3_lane"
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "reduce3_lane_scale"
  type: "Scale"
  bottom: "reduce3_lane"
  top: "reduce3_lane"
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "reduce3_lane_relu"
  type: "ReLU"
  bottom: "reduce3_lane"
  top: "reduce3_lane"
  relu_param {
    negative_slope: 0
  }
}
layer {
    name: "deconv3_lane"
    type: "Deconvolution"
    bottom: "reduce3_lane"
    top: "deconv3_lane"
    param{
        lr_mult: 1
        decay_mult: 1
    }
    convolution_param {
      kernel_size: 2  # {{2 * factor _ factor % 2}}  2 * 2 _ 0
      stride: 2       # {{factor}}
      num_output: 16 # {{C}} 
      #group: 48      # {{C}}
      pad: 0          # {{ceil((factor _ 1) / 2.)}} 2 _ 1 / 2
      weight_filler: { type: "xavier" } 
      bias_filler {
          type: "constant"
          value: 0
      }
    }
}

layer {
  name: "deconv3_lane_bn"
  type: "BatchNorm"
  bottom: "deconv3_lane"
  top: "deconv3_lane"
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "deconv3_lane_scale"
  type: "Scale"
  bottom: "deconv3_lane"
  top: "deconv3_lane"
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "deconv3_lane_relu"
  type: "ReLU"
  bottom: "deconv3_lane"
  top: "deconv3_lane"
  relu_param {
    negative_slope: 0
  }
}


########## end upsample 3 #####

######### upsample 4 ########

 layer {
  name: "reorg2"
  type: "Convolution"
  bottom: "conv2"
  top: "reorg2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 16
    bias_term: false
    kernel_size: 3
    pad: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
 }
#layer {
# name: "reorg2_bn"
# type: "BatchNorm"
# bottom: "reorg2"
# top: "reorg2"
# batch_norm_param {
#   eps: 1e-06
# }
#}
#layer {
# name: "reorg2_scale"
# type: "Scale"
# bottom: "reorg2"
# top: "reorg2"
# scale_param {
#   filler {
#     type: "constant"
#     value: 1
#   }
#   bias_term: true
# }
#}
layer {
  name: "reorg2_relu"
  type: "ReLU"
  bottom: "reorg2"
  top: "reorg2"
  relu_param {
    negative_slope: 0
  }
}


layer {
  name: "concat2"
  type: "Concat"
  bottom: "reorg2"
  bottom: "deconv3_lane"
  top: "concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "reduce4_lane"
  type: "Convolution"
  bottom: "concat2"
  top: "reduce4_lane"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "reduce4_lane_bn"
  type: "BatchNorm"
  bottom: "reduce4_lane"
  top: "reduce4_lane"
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "reduce4_lane_scale"
  type: "Scale"
  bottom: "reduce4_lane"
  top: "reduce4_lane"
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "reduce4_lane_relu"
  type: "ReLU"
  bottom: "reduce4_lane"
  top: "reduce4_lane"
  relu_param {
    negative_slope: 0
  }
}

layer {
    name: "deconv4_lane"
    type: "Deconvolution"
    bottom: "reduce4_lane"
    top: "deconv4_lane"
    param{
        lr_mult: 1
        decay_mult: 1
    }
    convolution_param {
      kernel_size: 2  # {{2 * factor _ factor % 2}}  2 * 2 _ 0
      stride: 2       # {{factor}}
      num_output: 8 # {{C}} 
      #group: 48      # {{C}}
      pad: 0          # {{ceil((factor _ 1) / 2.)}} 2 _ 1 / 2
      weight_filler: { type: "xavier" } 
      bias_filler {
          type: "constant"
          value: 0
      }
    }
}
layer {
  name: "deconv4_lane_bn"
  type: "BatchNorm"
  bottom: "deconv4_lane"
  top: "deconv4_lane"
  batch_norm_param {  
    use_global_stats:true  
    eps: 1e-06
  }
}
layer {
  name: "deconv4_lane_scale"
  type: "Scale"
  bottom: "deconv4_lane"
  top: "deconv4_lane"
  scale_param {
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
  }
}
layer {
  name: "deconv4_lane_relu"
  type: "ReLU"
  bottom: "deconv4_lane"
  top: "deconv4_lane"
  relu_param {
    negative_slope: 0
  }
}

layer {
  name: "reorg1"
  type: "Convolution"
  bottom: "conv1"
  top: "reorg1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 8
    bias_term: false
    kernel_size: 3
    pad: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
#layer {
# name: "reorg1_bn"
# type: "BatchNorm"
# bottom: "reorg1"
# top: "reorg1"
# batch_norm_param {
#   eps: 1e-06
# }
#}
#layer {
# name: "reorg1_scale"
# type: "Scale"
# bottom: "reorg1"
# top: "reorg1"
# scale_param {
#   filler {
#     type: "constant"
#     value: 1
#   }
#   bias_term: true
# }
#}
layer {
  name: "reorg1_relu"
  type: "ReLU"
  bottom: "reorg1"
  top: "reorg1"
  relu_param {
    negative_slope: 0
  }
}


layer {
  name: "concat1"
  type: "Concat"
  bottom: "reorg1"
  bottom: "deconv4_lane"
  top: "concat1"
  concat_param {
    axis: 1
  }
}

layer {
  name: "conv_out_lane"
  type: "Convolution"
  bottom: "concat1"
  top: "conv_out_lane"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 4
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}

layer {
  name: "conv_out_freespace"
  type: "Convolution"
  bottom: "concat1"
  top: "conv_out_freespace"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}

#layer {
#  name: "loss_lane"
#  type: "SoftmaxWithLoss"
#  bottom: "conv_out_lane"
#  bottom: "label_lane"
#  top: "loss"
#  loss_param {
#    normalization: FULL
#    label_weight: 1.0
#    label_weight: 20.0
#    label_weight: 20.0
#    label_weight: 20.0
# }
#  loss_weight: 5.0
#}#

#layer {
#  name: "loss_freespace"
#  type: "SoftmaxWithLoss"
#  bottom: "conv_out_freespace"
#  bottom: "label_freespace"
#  top: "loss_freespace"
#  loss_param {
#    normalization: FULL
#  }
#  loss_weight: 1.0
#}

layer {
  name: "out_lane"
  type: "Softmax"
  bottom: "conv_out_lane"
  top: "out_lane"
}

layer {
  name: "out_freespace"
  type: "Softmax"
  bottom: "conv_out_freespace"
  top: "out_freespace"
}
